<!DOCTYPE html>
<html lang="en">

<head>
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=UA-111713571-1"></script>
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());
		gtag('config', 'UA-111713571-1');
	</script>

	<!-- Required meta tags -->
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

	<!-- Bootstrap core CSS -->
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/css/bootstrap.min.css" integrity="sha384-/Y6pD6FV/Vv2HJnA6t+vslU6fwYXjCFtcEpHbNJ0lyAFsXTsjBbfaDjzALeQsN6M" crossorigin="anonymous">
	<!-- Custom styles for this template -->
	<link href="jumbotron.css" rel="stylesheet">
	<script src="js/main.js"></script>
	<script src="js/scroll.js"></script>
</head>

<title>Zeyu Yang</title>

<body>
	<nav class="navbar navbar-expand-md navbar-dark fixed-top bg-dark" id="Home">
		<a class="navbar-brand" href="#Home">Shuang Li</a>

		<div class="collapse navbar-collapse" id="navbarToggle">
			<ul class="navbar-nav ml-auto">
				<li class="nav-item">
					<a class="nav-link" href="#Home">Home</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="#Publications">Publications</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="#Contact">Contact</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="http://accessibility.mit.edu" target="_blank">Accessibility</a>
				</li>
			</ul>
		</div>
	</nav>

	<!-- <img class="img-responsive img-rounded" src="files/experience/shuang4_circle.jpg" alt="" style="max-width: 240px; border:1px solid black"><br> -->
	<div class="container" style="padding-top: 20px; font-size: 17px; width: 1080px">
		<div class="row">
			<div class="col-md-3", style="padding-right: 40px; padding-top: 25px">
				<!-- <img class="img-responsive img-rounded" src="files/experience/shuang4_circle.jpg" alt="" style="max-width: 240px; solid black"><br> -->
				<!-- <img class="img-responsive img-rounded" src="files/experience/shuang4.jpg" alt="" style="max-width: 220px; solid black"><br> -->
				<img class="img-responsive img-rounded" src="files/experience/shuangli-photo.JPG" alt="" style="max-width: 220px; solid black"><br>
			</div>


			<div class="col-md-4", style="padding-right: 40px; padding-top: 25px">
				<h2>Shuang Li</h2>
				<p>
					Fifth-year Ph.D. <br>
					MIT EECS<br>
					Computer Science & Artificial Intelligence Laboratory<br>
					Email: lishuang [at] mit (dot) edu<br><br>
					<a href="https://scholar.google.com/citations?user=auA1nNAAAAAJ" target="_blank">Google Scholar</a> /
					<a href="https://github.com/ShuangLI59" target="_blank">GitHub</a> /
					<a href="https://twitter.com/ShuangL13799063" target="_blank">Twitter</a>
				</p>
			</div>

			<div class="col-md-4", style="padding-right: 40px; padding-top: 25px">
                <font color="black"><b>Research Topics:</b><br>
			  		<a>Generative Modeling</a> <br>
                    <a>Foundation Models</a> <br>
					<a>Embodied AI</a> <br>
					<a>Language-Vision Understanding</a> <br>
                    <a>Compositionality</a> <br>
				</font>
			</div>
			
		</div>


			

		<div class="row">
			<div class="col-md-12">
			<br>
			<p>I am a fifth-year Ph.D. student at <a href="http://www.csail.mit.edu/" target="_blank">Computer Science and Artificial Intelligence Laboratory (CSAIL)</a> at  <a href="http://www.mit.edu/" target="_blank">MIT</a>, advised by Prof. <a href="http://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>. I also work closely with <a href="https://scholar.google.com/citations?user=Vzr1RukAAAAJ&hl=en" target="_blank">Igor Mordatch</a>.
            My research interests lie in <b>Artificial Intelligence</b>,  <b>Computer Vision</b>, <b>Decision Making</b>, and <b>Machine Learning</b>.
			In particular, I am interested in developing AI systems that exhibit generalization abilities to solve a wide range of tasks outside the training distribution.
			<!-- My research spans multiple areas, including Generative Modeling and Embodied AI.  -->
			<!-- I am interested in building intelligent agents that can continually learn and interact with the world around them. One aspect of my research is to teach AI systems to exhibit <a href="https://shuangli59.github.io/Learning-how-to-generalize/">Generalization</a> abilities. I hope my study on generalization will provide a possible solution for future AI systems to solve a wide range of real-world tasks. -->
            </p>

			</div>
		</div>
	</div><br>


  <!-- News -->
	<div class="container" style="padding-top: 20px; font-size: 17px; width: 1080px">
		<h3 id="News" style="padding-top: 80px; margin-top: -80px;">News</h3>
		<ul>
			<li>
				I am starting as an Assistant Professor at the <font color="firebrick"><b>University of Toronto</b></font> and <font color="firebrick"><b>Vector Institute</b></font> in Fall 2024!
			</li>
			<!-- <li>
				We are looking for students who are interested in <font color="firebrick"><b>Generative Models</b></font> (<a href="https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/"><b>Composable-Diffusion</b></a>, <a href="https://composevisualrelations.github.io/"><b>Composable-EBM</b></a>), <font color="firebrick"><b>Composing Foundation Models</b></font>, and <font color="firebrick"><b>Foundation Models for Decision-Making</b></font>.
			</li> -->
			<!-- <li>
				<font color="firebrick"><b>New</b></font> If you're an undergraduate or master student interested in research, please feel free to contact me. We are looking for outstanding students to engage in ongoing research projects.
			</li> -->
            <li>
				A summary of our recent works on <a href="https://shuangli59.github.io/compositional-generalization/"><font color="darkblue"><b>Compositional Generalization</b>.</font></a>
			</li>
			<li>
				A summary of our recent works on <a href="https://energy-based-model.github.io/Energy-based-Model-MIT/"><font color="darkblue"><b>Energy-based Models</b></font>.</a>
			</li>
			<li>
                I've been selected as one of the <a href="https://risingstars.utexas.edu/" target="_blank"><font color="darkblue"><b> EECS Rising Stars 2022</b></font>.</a>
            </li>
            <li>
                I've been selected as an <a href="https://icml.cc/Conferences/2022/Reviewers" target="_blank"><font color="darkblue"><b>Outstanding Reviewer for ICML 2022</b></font>.</a>
            </li>
			<li>
				I've been awarded <a href="https://research.fb.com/blog/2021/04/announcing-the-recipients-of-the-2021-facebook-fellowship-awards/" target="_blank"><font color="darkblue"><b>2021 Facebook Research Fellowship</b></font></a>. Thanks, Facebook!
			</li>
            <li>
				I've been awarded <a href="https://adoberesearch.ctlprojects.com/fellowship/previous-fellowship-award-winners/" target="_blank"><font color="darkblue"><b>2019 Adobe Research Fellowship</b></font></a>. Thanks, Adobe!
			</li>
		</ul>
	</div><br>




	<div class="container" style="padding-top: 20px; font-size: 17px; width: 1080px">
		<h3 id="talks" style="padding-top: 80px; margin-top: -80px;">Research Topics:</h3>
		We are looking for students who are interested in the following problems: <br><br>
		<ul>
			<li>
				<font color="firebrick"><b>Generative Modeling</b></font>
					<ul>
						<b>Video Generation / Understanding</b> <br>
						<ul>
							References: 
							<a href="https://runwayml.com/" target="_blank">Runway</a>,
							<a href="https://makeavideo.studio/" target="_blank">Make-A-Video</a>
						</ul>

						<b>Image Generation / Editing </b> <br>
						<ul>
							References: 
							<a href="https://github.com/Stability-AI/stablediffusion" target="_blank">Stable Diffusion</a>, Midjourney, 
							<a href="https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/" target="_blank">Composable-Diffusion</a>,
							<a href="https://composevisualrelations.github.io/" target="_blank">Composable-EBM</a>
						</ul>
					</ul>
			</li>

			<br>

			<li>
				<font color="firebrick"><b>Embodied AI</b></font>
					<ul>
						<b>Foundation Models for Decision-Making</b> <br>
						<ul>
							References: <br>
							<a href="https://arxiv.org/pdf/2304.03442.pdf" target="_blank">Generative Agents: Interactive Simulacra of Human Behavior</a> <br>
							<a href="https://fablestudio.github.io/showrunner-agents/" target="_blank">To Infinity and Beyond: SHOW-1 and Showrunner Agents in Multi-Agent Simulations</a> <br>
							<a href="https://shuangli-project.github.io/Pre-Trained-Language-Models-for-Interactive-Decision-Making/" target="_blank">Pre-Trained Language Models (LLM) for Interactive Decision-Making</a> <br>
						</ul>
					</ul>

					<ul>
						<b>Virtual Environment</b>
						<ul>
							References:
							<a href="http://virtual-home.org/" target="_blank">VirtualHome</a>,
							<a href="" target="_blank">VirtualCity (Coming soon)</a>
						</ul>
					</ul>
			</li>


			<br>

			<li>
				<font color="firebrick"><b>Foundation Models</b></font>
					<ul>
						<b>Composing Foundation Models</b> <br>
						<ul>
							References: <br>
							Through iterative refinement: <a href="https://energy-based-model.github.io/composing-pretrained-models-web/" target="_blank">Composing Ensembles of Pre-trained Models via Iterative Consensus</a> <br>
							Through multi-agent debate: <a href="https://composable-models.github.io/llm_debate/" target="_blank">Improving Factuality and Reasoning in Language Models through Multiagent Debate</a> <br>
							3D: <a href="https://concept-fusion.github.io/" target="_blank">ConceptFusion: Open-set Multimodal 3D Mapping</a> <br>
						</ul>
					</ul>
			</li>

		</ul>
	</div><br>

			<!-- <div class="col-md-4", style="padding-right: 40px; padding-top: 25px">
                <font color="black"><b></b><br>
			  		<a>Generative Modeling</a> <br>
                    <a>Foundation Models</a> <br>
					<a>Embodied AI</a> <br>
					<a>Language-Vision Understanding</a> <br>
                    <a>Compositional Generalization</a> <br>
				</font>
			</div> -->


	
	<div class="container" style="padding-top: 20px; font-size: 17px; width: 1080px">
		<h3 id="talks" style="padding-top: 80px; margin-top: -80px;">Recent Talks</h3>
		<ul>
			<li>
				[2023/8] Diffusion Models Course at SIGGRAPH
			</li>
			<li>
				[2023/5] Invited Talk at UC Berkeley
			</li>
			<li>
                [2023/4] Invited Talk at Nanyang Technological University
            </li>
            <li>
                [2023/2] Invited Talk at Columbia
            </li>
            <li>
				[2023/2] Invited Talk at UIUC
			</li>
			<li>
				[2023/2] Invited Talk at UT Austin
			</li>
			<li>
				[2022/12] Invited Talk at UC San Diego
			</li>
			<li>
				[2022/11] Invited Talk at Cornell Tech
			</li>
			<li>
				[2022/11] Invited Talk at UC Berkeley
			</li>
			<li>
				[2022/11] Invited Talk at Stanford
			</li>
			<li>
				[2022/11] Invited Talk at Northeastern
			</li>
		</ul>
	</div><br>



	<!-- Publications -->
	<div class="container" style="padding-top: 20px; font-size: 17px; width: 1080px">
		<h3 id="Datasets" style="padding-top: 80px; margin-top: -80px;">Datasets and Environments</h3>
		<ul>
			<li>
				Please send email to lishuang [at] mit [dot] edu to request for the datasets.
			</li>
			<li>
				<a href="http://virtual-home.org/">VirtualHome</a> multi-agent embodied environment for household activities
			</li>
			<li>
				<a href="https://shuangli-project.github.io/VHICO-Dataset/">V-HICO</a> dataset for video-based human-object interaction detection
			</li>
			<li>
				<a href="https://github.com/ShuangLI59/person_search">CUHK-SYSU</a> dataset for person re-identification
			</li>
			<li>
				<a href="https://github.com/ShuangLI59/Person-Search-with-Natural-Language-Description">CUHK-PEDES</a> dataset for natural language based person re-identification
			</li>
		</ul>
	</div><br>








<!-- 	<div class="container" style="padding-top: 20px; font-size: 17px; width: 1080px">
		<h3 id="Preprints" style="padding-top: 80px; margin-top: -80px;">Preprints</h3>

		<hr>
		<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="files/paper/23-fuse-models-3d/splash.gif" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<b><font color="black">ConceptFusion: Open-set Multimodal 3D Mapping</font></b><br>
				<a href="https://krrish94.github.io/" target="_blank">Krishna Murthy Jatavallabhula</a>,
				Alihusein Kuwajerwala, Qiao Gu, Mohd Omama, Tao Chen, 
				<a href="https://people.csail.mit.edu/lishuang/"><u><b>Shuang Li</b></u></a>,
				Ganesh Iyer, Soroush Saryazdi, Nikhil Keetha, Ayush Tewari, Joshua B. Tenenbaum, Celso Miguel de Melo, Madhava Krishna, Liam Paull, Florian Shkurti, and Antonio Torralba
				<br>
				<b><a href="" target="_blank">arXiv 2023</a></b> <br>
				<a href="https://concept-fusion.github.io/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/abs/2302.07241" target="_blank"> <small>[Paper]</small></a>
				<a href="" target="_blank"> <small>[Github]</small></a>
                </div>
		</div><hr>
	</div><br>

 -->



	<!-- Publications -->
	<div class="container" style="padding-top: 20px; font-size: 17px; width: 1080px">
		<h3 id="Publications" style="padding-top: 80px; margin-top: -80px;">
			Recent Publications
			<span style="float:right;"><a href="https://scholar.google.com/citations?user=auA1nNAAAAAJ">[Full List]</a></span>
		</h3>


		<div id="pubs"></div>


		<!-- ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ -->


		<script id="pubs_by_date" language="text">
			<hr>
		  	<font color="black">(* indicates equal contribution)</font>
		  	<br><br>


			



			  <div class="row">
				<div class="col-md-3">
					<video width="100%" playsinline="" autoplay="" loop="" preload="" muted="" style="border:1px solid black">
                		<source src="files/paper/23-llm-debate/debate.mp4" type="video/mp4">
            		</video>
				</div>
				<div class="col-md-9">
				<b><font color="black">Improving Factuality and Reasoning in Language Models through Multiagent Debate</font></b><br>
				<a href="https://yilundu.github.io/" target="_blank">Yilun Du</a>,
				<a href="https://people.csail.mit.edu/lishuang/"><u><b>Shuang Li</b></u></a>,
				<a href="https://groups.csail.mit.edu/vision/torralbalab/" target="_blank">Antonio Torralba</a>,
				<a href="https://scholar.google.com/citations?user=rRJ9wTJMUB8C&hl=en" target="_blank">Joshua B. Tenenbaum</a>, and
				<a href="https://scholar.google.com/citations?user=Vzr1RukAAAAJ&hl=en" target="_blank">Igor Mordatch</a>
				<br>
				<b><a href="https://roboticsconference.org/" target="_blank">arXiv 2023</a></b> <br>
				<a href="https://composable-models.github.io/llm_debate/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/abs/2305.14325" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/composable-models/llm_multiagent_debate" target="_blank"> <small>[Code]</small></a>
                </div>
			</div><hr>




			<div class="row">
				<div class="col-md-3">
					<video width="100%" playsinline="" autoplay="" loop="" preload="" muted="" style="border:1px solid black">
                		<source src="files/paper/23-unsupervised-dm/unsup_discovery.m4v" type="video/mp4">
            		</video>
				</div>
				<div class="col-md-9">
				<b><font color="black">Unsupervised Compositional Concepts Discovery with Text-to-Image Generative Models</font></b><br>
				<a href="https://nanliu.io/" target="_blank">Nan Liu*</a>, 
				<a href="https://yilundu.github.io/" target="_blank">Yilun Du*</a>,
				<a href="https://people.csail.mit.edu/lishuang/"><u><b>Shuang Li*</b></u></a>,
				<a href="https://scholar.google.com/citations?user=rRJ9wTJMUB8C&hl=en" target="_blank">Joshua B. Tenenbaum</a>, and
				<a href="https://groups.csail.mit.edu/vision/torralbalab/" target="_blank">Antonio Torralba</a>
				<br>
				<b><a href="https://iccv2023.thecvf.com/" target="_blank">ICCV 2023</a></b> <br>
				<a href="https://energy-based-model.github.io/unsupervised-concept-discovery/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/abs/2306.05357.pdf" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/nanlliu/Unsupervised-Compositional-Concepts-Discovery" target="_blank"> <small>[Code]</small></a>
                </div>
			</div><hr>





			<div class="row">
				<div class="col-md-3">
					<video width="100%" playsinline="" autoplay="" loop="" preload="" muted="" style="border:1px solid black">
                		<source src="files/paper/23-iccv-segmentation/image1.mp4" type="video/mp4">
            		</video>
				</div>
				<div class="col-md-9">
				<b><font color="black">Open-vocabulary Panoptic Segmentation with Embedding Modulation</font></b><br>
				<a href="https://xavierchen34.github.io/" target="_blank">Xi Chen</a>, 
				<a href="https://people.csail.mit.edu/lishuang/"><u><b>Shuang Li</b></u></a>,
				<a href="https://ai.meta.com/people/ser-nam-lim/" target="_blank">Ser-Nam Lim</a>,
				<a href="https://groups.csail.mit.edu/vision/torralbalab/" target="_blank">Antonio Torralba</a>, and
				<a href="https://hszhao.github.io/" target="_blank">Hengshuang Zhao</a>
				<br>
				<b><a href="https://iccv2023.thecvf.com/" target="_blank">ICCV 2023</a></b> <br>
				<a href="https://opsnet-page.github.io/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/pdf/2303.11324.pdf" target="_blank"> <small>[Paper]</small></a>
                </div>
			</div><hr>

			

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="files/paper/23-fuse-models-3d/splash.gif" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<b><font color="black">ConceptFusion: Open-set Multimodal 3D Mapping</font></b><br>
				<a href="https://krrish94.github.io/" target="_blank">Krishna Murthy Jatavallabhula</a>,
				Alihusein Kuwajerwala, Qiao Gu, Mohd Omama, Tao Chen, 
				<a href="https://people.csail.mit.edu/lishuang/"><u><b>Shuang Li</b></u></a>,
				Ganesh Iyer, Soroush Saryazdi, Nikhil Keetha, Ayush Tewari, Joshua B. Tenenbaum, Celso Miguel de Melo, Madhava Krishna, Liam Paull, Florian Shkurti, and Antonio Torralba
				<br>
				<b><a href="https://roboticsconference.org/" target="_blank">RSS 2023</a></b> <br>
				<b><a href="https://cvpr2023.thecvf.com/Conferences/2023/CallForDemos" target="_blank">CVPR 2023 Demos (and Art Gallery)</a></b> <br>
				<b><a href="https://microsoft.github.io/robotics.pretraining.workshop.icra/" target="_blank">ICRA Pretraining for Robotics Workshop 2023</a></b> <br>
				<a href="https://concept-fusion.github.io/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/abs/2302.07241" target="_blank"> <small>[Paper]</small></a>
				<a href="https://www.youtube.com/watch?v=rkXgws8fiDs" target="_blank"> <small>[Video]</small></a>
                </div>
			</div><hr>




			<div class="row">
				<div class="col-md-3">
					<video width="100%" playsinline="" autoplay="" loop="" preload="" muted="" style="border:1px solid black">
                		<source src="files/paper/22-compose-pretrained-models/demo-small-small.mp4" type="video/mp4">
            		</video>
				</div>
				<div class="col-md-9">
				<b><font color="black">Composing Ensembles of Pre-trained Models via Iterative Consensus</font></b><br>
				<a href="https://people.csail.mit.edu/lishuang/"><u><b>Shuang Li</b></u></a>,
				<a href="https://yilundu.github.io/" target="_blank">Yilun Du</a>,
				<a href="https://scholar.google.com/citations?user=rRJ9wTJMUB8C&hl=en" target="_blank">Joshua B. Tenenbaum</a>,
				<a href="https://groups.csail.mit.edu/vision/torralbalab/" target="_blank">Antonio Torralba</a>, and
				<a href="https://scholar.google.com/citations?user=Vzr1RukAAAAJ&hl=en" target="_blank">Igor Mordatch</a>
				<br>
				<b><a href="https://iclr.cc/" target="_blank">ICLR 2023</a></b> <br>
				<a href="https://energy-based-model.github.io/composing-pretrained-models-web/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/abs/2210.11522" target="_blank"> <small>[Paper]</small></a>
				<a href="" target="_blank"> <small>[Github]</small></a>
                </div>
			</div><hr>




			<div class="row">
				<div class="col-md-3">
					<video width="100%" playsinline="" autoplay="" loop="" preload="" muted="" style="border:1px solid black">
                		<source src="files/paper/21-virtualhome-language/vid1.mp4" type="video/mp4">
            		</video>
				</div>
				<div class="col-md-9">
				<b><font color="black">Pre-Trained Language Models for Interactive Decision-Making</font></b><br>
				<a href="https://people.csail.mit.edu/lishuang/"><u><b>Shuang Li</b></u></a>,
				<a href="https://people.csail.mit.edu/xavierpuig/" target="_blank">Xavier Puig</a>,
				<a href="" target="_blank">Chris Paxton</a>,
				<a href="https://yilundu.github.io/" target="_blank">Yilun Du</a>,
				<a href="https://clintonjwang.github.io/" target="_blank">Clinton Wang</a>,
				<a href="" target="_blank">Linxi Fan</a>,
				<a href="https://taochenshh.github.io" target="_blank">Tao Chen</a>,
				<a href="" target="_blank">De-An Huang</a>,
				<a href="" target="_blank">Ekin Akyürek</a>, 
				<a href="" target="_blank">Anima Anandkumar<sup>+</sup></a>,
				<a href="" target="_blank">Jacob Andreas<sup>+</sup></a>,
				<a href="https://scholar.google.com/citations?user=Vzr1RukAAAAJ&hl=en" target="_blank">Igor Mordatch<sup>+</sup></a>, 
				<a href="https://groups.csail.mit.edu/vision/torralbalab/" target="_blank">Antonio Torralba<sup>+</sup></a>, and
				<a href="" target="_blank">Yuke Zhu<sup>+</sup></a> <br>
				Junior authors are ordered based on their contributions and senior authors<sup>+</sup> are ordered alphabetically
				<br>
				<b><a href="https://nips.cc/" target="_blank">NeurIPS 2022</a>, <font color="firebrick">Oral</font> </b> <br>
				<a href="https://shuangli-project.github.io/Pre-Trained-Language-Models-for-Interactive-Decision-Making/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/abs/2202.01771" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/ShuangLI59/Pre-Trained-Language-Models-for-Interactive-Decision-Making" target="_blank"> <small>[Code]</small></a>
				</div>
			</div><hr>





			<div class="row">
				<div class="col-md-3">
					<video width="100%" playsinline="" autoplay="" loop="" preload="" muted="" style="border:1px solid black">
                		<source src="files/paper/22-compose-duffusion-model/webpage-teaser2.mp4" type="video/mp4">
            		</video>
				</div>
				<div class="col-md-9">
				<b><font color="black">Compositional Visual Generation with Composable Diffusion Models</font></b><br>
				<a href="https://nanliu.io/" target="_blank">Nan Liu*</a>,
				<a href="https://people.csail.mit.edu/lishuang/"><u><b>Shuang Li*</b></u></a>,
				<a href="https://yilundu.github.io/" target="_blank">Yilun Du*</a>,
				<a href="https://groups.csail.mit.edu/vision/torralbalab/" target="_blank">Antonio Torralba</a>, and
				<a href="https://scholar.google.com/citations?user=rRJ9wTJMUB8C&hl=en" target="_blank">Joshua B. Tenenbaum</a>
				(*equal contribution)
				<br>
				<b><a href="https://eccv2022.ecva.net/" target="_blank">ECCV 2022</a></b> <br>
				<a href="https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/pdf/2206.01714.pdf" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/energy-based-model/Compositional-Visual-Generation-with-Composable-Diffusion-Models-PyTorch" target="_blank"> <small>[Github]</small></a>
				<a href="https://colab.research.google.com/github/energy-based-model/Compositional-Visual-Generation-with-Composable-Diffusion-Models-PyTorch/blob/main/notebooks/compose_glide.ipynb" target="_blank"> <small>[Colab]</small></a>
				<a href="https://huggingface.co/spaces/Shuang59/Composable-Diffusion" target="_blank"> <small>[Demo]</small></a>
                <br>
                <a> Press coverage: <a href="https://news.mit.edu/2022/ai-system-makes-models-like-dall-e-2-more-creative-0908" target="_blank">MIT News</a>, <a href="https://www.csail.mit.edu/news/ai-system-makes-models-dall-e-2-more-creative" target="_blank">MIT CSAIL News</a> </a>
                </div>
			</div><hr>



			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="files/paper/21-iclr-continual-learning/fig2.png" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<b><font color="black">Energy-Based Models for Continual Learning</font></b><br>
				<a href="https://people.csail.mit.edu/lishuang/"><u><b>Shuang Li</b></u></a>,
				<a href="https://yilundu.github.io/" target="_blank">Yilun Du</a>,
				<a href="https://scholar.google.com/citations?user=3k0l15MAAAAJ&hl=en" target="_blank">Gido M. van de Ven</a>, and
				<a href="https://scholar.google.com/citations?user=Vzr1RukAAAAJ&hl=en" target="_blank">Igor Mordatch</a>
				<br>
				<b><a href="https://lifelong-ml.cc/" target="_blank">CoLLAs 2022</a>, <font color="firebrick">Oral</font> </b> <br>
				<b><a href="https://sites.google.com/view/ebm-workshop-iclr2021" target="_blank">ICLR EBM Workshop 2021</a>, <font color="firebrick">Oral</font> </b> <br>
				<a href="https://energy-based-model.github.io/Energy-Based-Models-for-Continual-Learning/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/pdf/2011.12216.pdf" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/ShuangLI59/ebm-continual-learning" target="_blank"> <small>[Code]</small></a>
				</div>
			</div><hr>





		  	<div class="row">
				<div class="col-md-3">
					<video width="100%" playsinline="" autoplay="" loop="" preload="" muted="" style="border:1px solid black">
                		<source src="files/paper/21-neurips-compose-relation/clevr_teaser.mp4" type="video/mp4">
            		</video>
				</div>
				<div class="col-md-9">
				<b><font color="black">Learning to Compose Visual Relations</font></b><br>
				<a href="https://nanliu.io/" target="_blank">Nan Liu*</a>,
				<a href="https://people.csail.mit.edu/lishuang/"><u><b>Shuang Li*</b></u></a>,
				<a href="https://yilundu.github.io/" target="_blank">Yilun Du*</a>,
				<a href="https://scholar.google.com/citations?user=rRJ9wTJMUB8C&hl=en" target="_blank">Joshua B. Tenenbaum</a>, and
				<a href="https://groups.csail.mit.edu/vision/torralbalab/" target="_blank">Antonio Torralba</a>
				(*equal contribution)
				<br>
				<b><a href="https://neurips.cc/" target="_blank">NeurIPS 2021</a>, <font color="firebrick">Spotlight</font> </b> <br>
				<b><a href="https://ctrlgenworkshop.github.io/accepted_papers.html" target="_blank">NeurIPS Controllable Generative Modeling Workshop 2021</a>, <font color="firebrick">Outstanding Paper Award</font> </b> <br>
				<a href="https://composevisualrelations.github.io/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/abs/2111.09297" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/nanlliu/compose-visual-relations" target="_blank"> <small>[Code]</small></a> <br>
                <a> Press coverage: <a href="https://news.mit.edu/2021/ai-object-relationships-image-generation-1129" target="_blank">MIT News</a>, <a href="https://www.csail.mit.edu/news/artificial-intelligence-understands-object-relationships" target="_blank">MIT CSAIL News</a> </a>
				</div>
			</div><hr>



			

		  	<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="files/paper/21-nerf-dy/nerf-dy.gif" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<b><font color="black">3D Neural Scene Representations for Visuomotor Control</font></b><br>
				<a href="https://people.csail.mit.edu/liyunzhu/" target="_blank">Yunzhu Li*</a>,
				<a href="https://people.csail.mit.edu/lishuang/"><u><b>Shuang Li*</b></u></a>,
				<a href="https://vsitzmann.github.io/" target="_blank">Vincent Sitzmann</a>, 
				<a href="https://people.csail.mit.edu/pulkitag/" target="_blank">Pulkit Agrawal</a>, and
				<a href="https://groups.csail.mit.edu/vision/torralbalab/" target="_blank">Antonio Torralba</a>
				(*equal contribution)
                <br>
				<b><a href="https://www.robot-learning.org/" target="_blank">CoRL 2021</a>, <font color="firebrick">Oral</font> </b> <br>
				<b><a href="https://rssvlrr.github.io/" target="_blank">RSS Visual Learning and Reasoning for Robotics Workshop 2021</a></b> <br>
				<a href="https://3d-representation-learning.github.io/nerf-dy/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/abs/2107.04004" target="_blank"> <small>[Paper]</small></a>
				<a href="" target="_blank"> <small>[Code]</small></a>
				</div>
			</div><hr>



             <div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="files/paper/21-iclr-watch-and-help/multiagent.gif" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<b><font color="black">Watch-And-Help: A Challenge for Social Perception and Human-AI Collaboration</font></b><br>
				<a href="https://people.csail.mit.edu/xavierpuig/" target="_blank">Xavier Puig</a>,
				<a href="https://www.tshu.io/" target="_blank">Tianmin Shu</a>,
				<a href="https://people.csail.mit.edu/lishuang/"><u><b>Shuang Li</b></u></a>,
				<a href="" target="_blank">Zilin Wang</a>,
				<a href="https://scholar.google.com/citations?user=rRJ9wTJMUB8C&hl=en" target="_blank">Joshua B. Tenenbaum</a>, 
				<a href="https://www.cs.utoronto.ca/~fidler/" target="_blank">Sanja Fidler</a>, and
				<a href="https://groups.csail.mit.edu/vision/torralbalab/" target="_blank">Antonio Torralba</a>
				<br>
				<b><a href="https://iclr.cc" target="_blank">ICLR 2021</a>, <font color="firebrick">Spotlight</font> </b> <br>
				<b><a href="https://www.cooperativeai.com/neurips-2020" target="_blank">NeurIPS Cooperative AI Workshop 2020</a>, <font color="firebrick">Best Paper Award</font> </b> <br>
				<a href="http://virtual-home.org/watch_and_help/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/pdf/2010.09890.pdf" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/xavierpuigf/watch_and_help" target="_blank"> <small>[Code]</small></a>
				<a href="https://www.youtube.com/watch?v=lrB4K2i8xPI&feature=youtu.be" target="_blank"> <small>[Video]</small></a>
				<a href="http://virtual-home.org/" target="_blank"> <small>[VirtualHome]</small></a>
				</div>
			</div><hr>


			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="files/paper/20-neurips-ebm-compositional/comp-face.gif" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<b><font color="black">Compositional Visual Generation with Energy Based Models</font></b><br>
				<a href="https://yilundu.github.io/" target="_blank">Yilun Du</a>,
				<a href="https://people.csail.mit.edu/lishuang/"><u><b>Shuang Li</b></u></a>, and
				<a href="https://scholar.google.com/citations?user=Vzr1RukAAAAJ&hl=en" target="_blank">Igor Mordatch</a>
				<br>
				<b><a href="https://neurips.cc/" target="_blank">NeurIPS 2020</a>, <font color="firebrick">Spotlight</font> </b> <br>
				<a href="https://energy-based-model.github.io/compositional-generation-inference/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/pdf/2004.06030.pdf" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/yilundu/ebm_compositionality" target="_blank"> <small>[Code]</small></a>
				</div>
			</div><hr>



			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="files/paper/17-iccv-pose/17-iccv-pose-v1.png" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<b><font color="black">Learning Feature Pyramids for Human Pose Estimation</font></b><br>
				<a href="https://scholar.google.com/citations?user=6QQX88UAAAAJ&hl=en" target="_blank">Wei Yang</a>,
				<a href="https://people.csail.mit.edu/lishuang/"><u><b>Shuang Li</b></u></a>,
				<a href="https://scholar.google.com/citations?user=pw_0Z_UAAAAJ&hl=en" target="_blank">Wanli Ouyang</a>,
				<a href="https://scholar.google.com/citations?user=BN2Ze-QAAAAJ&hl=en" target="_blank">Hongsheng Li</a>, and
				<a href="https://scholar.google.com/citations?user=-B5JgjsAAAAJ&hl=en" target="_blank">Xiaogang Wang</a>
				<br>
				<b><a href="https://iccv2017.thecvf.com/" target="_blank">ICCV 2017</a></b> <br>
				<a href="https://arxiv.org/pdf/1708.01101.pdf" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/bearpaw/PyraNet" target="_blank"> <small>[Code]</small></a>
				</div>
			</div><hr>



			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="files/paper/17-cvpr-nlp/17-cvpr-nlp-4.png" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<b><font color="black">Person Search with Natural Language Description</font></b><br>
				<a href="https://people.csail.mit.edu/lishuang/"><u><b>Shuang Li</b></u></a>,
				<a href="https://scholar.google.com/citations?user=DGHphywAAAAJ&hl=en" target="_blank">Tong Xiao</a>,
				<a href="https://scholar.google.com/citations?user=BN2Ze-QAAAAJ&hl=en" target="_blank">Hongsheng Li</a>,
				<a href="http://bzhou.ie.cuhk.edu.hk/" target="_blank">Bolei Zhou</a>,
				<a href="" target="_blank">Dayu Yue</a>, and
				<a href="https://scholar.google.com/citations?user=-B5JgjsAAAAJ&hl=en" target="_blank">Xiaogang Wang</a>
				<br>
				<b><a href="https://cvpr2017.thecvf.com/" target="_blank">CVPR 2017</a></b> <br>
				<a href="https://arxiv.org/pdf/1702.05729.pdf" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/ShuangLI59/Person-Search-with-Natural-Language-Description" target="_blank"> <small>[Code]</small></a>
				</div>
			</div><hr>




			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="files/paper/17-cvpr-person-search/17-cvpr-person-search-v1.png" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<b><font color="black">Joint Detection and Identification Feature Learning for Person Search</font></b><br>
				<a href="https://scholar.google.com/citations?user=DGHphywAAAAJ&hl=en" target="_blank">Tong Xiao*</a>,
				<a href="https://people.csail.mit.edu/lishuang/"><u><b>Shuang Li*</b></u></a>,
				<a href="https://scholar.google.com/citations?user=hmGZN08AAAAJ&hl=en" target="_blank">Bochao Wang</a>,
				<a href="https://scholar.google.com/citations?user=Nav8m8gAAAAJ&hl=en" target="_blank">Liang Lin</a>, and
				<a href="https://scholar.google.com/citations?user=-B5JgjsAAAAJ&hl=en" target="_blank">Xiaogang Wang</a>
				(*equal contribution)
				<br>
				<b><a href="https://cvpr2017.thecvf.com/" target="_blank">CVPR 2017</a>, <font color="firebrick">Spotlight</font> </b> <br>
				<a href="https://arxiv.org/pdf/1604.01850.pdf" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/ShuangLI59/person_search" target="_blank"> <small>[Code]</small></a>
				</div>
			</div><hr>


		

			
		</script>


	</div><br>







	<div class="container" style="padding-top: 20px; font-size: 17px; width: 1080px">
		<h3 id="Honors" style="padding-top: 80px; margin-top: -80px;">Selected Honors</h3>
		<ul>
            <li>EECS Rising Stars, 2022 <a href="https://risingstars.utexas.edu/" target="_blank"><small>[Link]</small></a></li>
            <li>ICML Outstanding Reviewer, 2022 <a href="https://icml.cc/Conferences/2022/Reviewers" target="_blank"><small>[Link]</small></a></li>
			<li>Facebook Research Fellowship, 2021 <a href="https://research.fb.com/blog/2021/04/announcing-the-recipients-of-the-2021-facebook-fellowship-awards/" target="_blank"><small>[Link]</small></a></li>
			<li>Adobe Research Fellowship, 2019 <a href="https://adoberesearch.ctlprojects.com/fellowship/previous-fellowship-award-winners/" target="_blank"><small>[Link]</small></a></li>
			<li>MIT Seneff-Zue CS Fellowship, 2018 </a></li>
            <li>Outstanding Students Award, CUHK, 2018 </a></li>
        </ul>
	</div><br>

	
	<!-- Teaching -->
	<div class="container" style="padding-top: 20px; font-size: 17px; width: 1080px">
		<h3 id="teaching" style="padding-top: 80px; margin-top: -80px;">Teaching</h3>
		<ul>
			<li>6.819/6.869: Advances in Computer Vision, MIT</a></li>
			<li>ENGG 2420B: Complex Analysis and Differential Equations for Engineers, CUHK</li>
			<li>ELEG 1110: Problem Solving By Programming, CUHK</li>
			<li>ELEG 2201: Digital Circuits and Systems, CUHK</li>
        </ul>
	</div><br>



	<!-- Service -->
	<div class="container" style="padding-top: 20px; font-size: 17px; width: 1080px">
		<h3 id="Service" style="padding-top: 80px; margin-top: -80px;">Professional Service</h3>
		<ul>
			<li> Workshops / Tutorials: <br>
				<a href="https://s2023.siggraph.org/presentation/?id=gensub_208&sess=sess164" target="_blank">Diffusion Models Course at SIGGRAPH 2023 <br>
				<a href="https://social-intelligence-human-ai.github.io/" target="_blank">Social Intelligence in Humans and Robots</a> at ICRA 2021, RSS 2022, RSS 2023 <br>
				<a href="http://wider-challenge.org/2019.html" target="_blank">Wider Face and Person Challenge</a> at ICCV 2019 <br>
				<a href="http://vc-workshop.csail.mit.edu" target="_blank">MIT Visual Computing Workshop</a> 2021 <br>
			</li>
			<li> Conference Reviewer: <br>
				IEEE Conference on Computer Vision and Pattern Recognition (CVPR) <br>
				International Conference on Computer Vision (ICCV) <br>
				European Conference on Computer Vision (ECCV) <br>
				ACM International Conference on Multimedia (ACMMM) <br>
				International Conference on Machine Learning (ICML) <br>
				Conference on Neural Information Processing Systems (NeurIPS) <br>
				International Conference on Learning Representations (ICLR) <br>
			</li>
			<li>Journal Reviewer: <br>
				IEEE Transactions on Image Processing (TIP) <br>
				IEEE Transactions on Circuits and Systems for Video Technology (TCSVT) <br>
				IEEE Transactions on Cybernetics <br>
				Computer Vision and Image Understanding (CVIU) <br>
				Image and Vision Computing (IMAVIS) <br>
			</li>
		</ul>
	</div><br>



	

	<!-- Contact -->
	<div class="container" style="padding-top: 20px; font-size: 17px; width: 1080px">
		<h3 id="Contact" style="padding-top: 80px; margin-top: -80px;">Contact</h3>
		MIT Computer Science and Artificial Intelligence Lab <br>
		32 Vassar Street, 32-D394 <br>
		Cambridge, MA 02139 <br>
	</div>

	<div class="container">
		<hr>
		<center>
			<footer>
				<p>&copy; Massachusetts Institute of Technology 2020</p>
			</footer>
		</center>
	</div>
	<!-- /container -->

	<!-- Bootstrap core JavaScript -->
	<!-- Placed at the end of the document so the pages load faster -->
	<script>showPubs(0);</script>
	<script>var scroll = new SmoothScroll('a[href*="#"]', {speed: 1000});</script>
	<script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.11.0/umd/popper.min.js" integrity="sha384-b/U6ypiBEHpOf/4+1nzFpr53nxSS+GLCkfwBdFNTxtclqqenISfwAzpKaMNFNmj4" crossorigin="anonymous"></script>
	<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/js/bootstrap.min.js" integrity="sha384-h0AbiXch4ZDo7tp9hKZ4TsHbi047NrKGLO3SEJAg45jXxnGIfYzk4Si90RDIqNm1" crossorigin="anonymous"></script>
</body>

</html>
