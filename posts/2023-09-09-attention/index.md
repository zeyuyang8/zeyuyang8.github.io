# Attention

## Introduction

Attention is a mechanism that allows a neural network to focus on certain parts of an input sequence when producing an output sequence.

## Types of attention

- Global attention
- Local attention
- Soft attention
- Hard attention

## Attention in diffusing models

- Self attention
- Cross attention
